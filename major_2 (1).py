# -*- coding: utf-8 -*-
"""Major 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1esPoGwn5f2LY77qFAngNEaSGvwsizsm5
"""

import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
from tensorflow.keras import mixed_precision

# Speed Boost: Use float16 precision for faster training
mixed_precision.set_global_policy('mixed_float16')

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Normalize pixel values to 0–1
x_train = x_train / 255.0
x_test = x_test / 255.0

def resize_images(images):
    resized = []
    for img in images:
        img_resized = tf.image.resize(img, (64, 64))
        resized.append(img_resized)
    return tf.stack(resized)

x_train_resized = resize_images(x_train)
x_test_resized = resize_images(x_test)

y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# Freeze all layers except last 4 conv layers for fine-tuning
for layer in base_model.layers[:-4]:
    layer.trainable = False
for layer in base_model.layers[-4:]:
    layer.trainable = True

x = Flatten()(base_model.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(10, activation='softmax', dtype='float32')(x)  # dtype fix for mixed precision

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)
datagen.fit(x_train_resized)

# Early stopping to avoid overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Learning rate schedule for better convergence
def scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

lr_schedule = LearningRateScheduler(scheduler)

from tensorflow.keras.callbacks import ReduceLROnPlateau

# Define the ReduceLROnPlateau callback
lr_reduce = ReduceLROnPlateau(
    monitor='val_loss',       # Monitor validation loss
    factor=0.5,               # Reduce learning rate by this factor
    patience=3,               # Wait 3 epochs before reducing
    verbose=1,                # Print the learning rate changes
    min_lr=1e-6               # Do not reduce below this learning rate
)

# Step 9: Train the model for 70 full epochs without EarlyStopping
history = model.fit(
    datagen.flow(x_train_resized, y_train_cat, batch_size=64),  # Using ImageDataGenerator
    validation_data=(x_test_resized, y_test_cat),               # Validation set
    epochs=80,                                                  # Train for 80 epochs
    callbacks=[lr_reduce],                                      # Only learning rate reduction
    verbose=1                                                   # Show training progress
)

test_loss, test_acc = model.evaluate(x_test_resized, y_test_cat)
print(f"✅ Final Test Accuracy: {test_acc * 100:.2f}%")

import matplotlib.pyplot as plt

# Plot training & validation accuracy
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='green')
plt.title('Model Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()

# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', color='red')
plt.plot(history.history['val_loss'], label='Val Loss', color='orange')
plt.title('Model Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(x_test_resized, y_test_cat, verbose=0)
print(f"✅ Final Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"❌ Final Test Loss: {test_loss:.4f}")

import time
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
import matplotlib.pyplot as plt

# Load test data
(_, _), (x_test, _) = cifar10.load_data()
x_test = x_test.astype("float32") / 255.0

# Resize only the first 100 images to 224x224
x_test_resized = tf.image.resize(x_test[:100], [224, 224])
sample_images = preprocess_input(x_test_resized.numpy())

# Load pre-trained MobileNetV2 model
model = MobileNetV2(weights='imagenet', include_top=True)

# Measure inference time
start = time.time()
_ = model.predict(sample_images)
end = time.time()
mobilenet_delay = end - start

print(f"✅ MobileNetV2 Inference Time for 100 images: {mobilenet_delay:.4f} seconds")

# Initial data
systems = ["Intel Core i3 CPU", "MobileNetV2 (GPU)", "Intel Haswell CPU", "NVIDIA GPU", "VGG-16 (GPU)"]
times = [12.6, mobilenet_delay, 10.2, 1.3, 0.9]
colors = ["gray", "lightblue", "orange", "green", "purple"]

# Zip and sort by times descending
combined = list(zip(times, systems, colors))
combined.sort(reverse=True)
times, systems, colors = zip(*combined)

# Plot
plt.figure(figsize=(10, 6))
bars = plt.bar(systems, times, color=colors)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.2, f"{yval:.2f}", ha='center', fontsize=12)

plt.title("Inference Time Comparison (Descending Order - Higher is Worse)", fontsize=14)
plt.ylabel("Time (in seconds)")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# ✅ Install dependencies
!pip install -q gradio tensorflow

# ✅ Imports
import gradio as gr
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import decode_predictions, preprocess_input
from tensorflow.keras.preprocessing import image

# ✅ Load MobileNetV2 model (pretrained on ImageNet)
model = MobileNetV2(weights='imagenet')

# ✅ Prediction function
def classify_image(img):
    # Resize to 224x224 as required by MobileNetV2
    img = tf.image.resize(img, (224, 224))
    img_array = tf.expand_dims(img, axis=0)  # Add batch dimension
    img_array = preprocess_input(img_array)  # Preprocess for MobileNetV2

    # Predict
    preds = model.predict(img_array)
    decoded = decode_predictions(preds, top=1)[0][0]  # Get top-1 class
    label = decoded[1]  # Class name
    confidence = float(decoded[2]) * 100

    return {label: confidence}

# ✅ Gradio UI
interface = gr.Interface(
    fn=classify_image,
    inputs=gr.Image(type="numpy"),
    outputs=gr.Label(num_top_classes=1),
    title="Real-World Image Classifier (MobileNetV2)",
    description="Upload any image and get classification using MobileNetV2 pretrained on ImageNet"
)

# ✅ Launch the app
interface.launch(share=True)

# ✅ Install required packages
!pip install -q tensorflow tensorflow-datasets pandas openpyxl
!pip install -q qgrid

# ✅ Imports
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
from PIL import Image
import pandas as pd
from io import BytesIO
import base64
import IPython
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions

# ✅ Load model
model = MobileNetV2(weights='imagenet')

# ✅ Class keywords
dog_keywords = ['dog', 'terrier', 'hound', 'retriever', 'poodle', 'beagle', 'pinscher', 'Chihuahua', 'Cardigan']
cat_keywords = ['cat', 'tabby', 'Egyptian']

def classify_label(predicted_label):
    label = predicted_label.lower()
    if any(word in label for word in dog_keywords):
        return "Dog"
    elif any(word in label for word in cat_keywords):
        return "Cat"
    else:
        return "None"

def image_to_base64(img):
    buffered = BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return f'<img src="data:image/png;base64,{img_str}" width="64"/>'

# ✅ Load sample images from TensorFlow Datasets
ds = tfds.load('cats_vs_dogs', split='train[:10]', as_supervised=True)

# ✅ Process images with rotation and prediction
angles = list(range(0, 360, 45))
rows = []

for idx, (image, label) in enumerate(ds):
    img = tf.image.resize(image, (224, 224)).numpy().astype(np.uint8)
    pil_img = Image.fromarray(img)
    thumbnail = image_to_base64(pil_img)

    for angle in angles:
        rotated = pil_img.rotate(angle)
        arr = np.expand_dims(np.array(rotated), axis=0)
        arr = preprocess_input(arr)

        preds = model.predict(arr)
        decoded = decode_predictions(preds, top=1)[0][0]
        pred_label = decoded[1]
        confidence = decoded[2] * 100
        pred_class = classify_label(pred_label)

        rows.append({
            "Image": thumbnail,
            "Image Index": idx,
            "Rotation": angle,
            "Predicted Label": pred_label,
            "Predicted Class": pred_class,
            "Confidence (%)": f"{confidence:.2f}%"
        })

# ✅ Create DataFrame
df = pd.DataFrame(rows)

# ✅ Style for display
def color_class(val):
    color = {"Dog": "green", "Cat": "blue", "None": "gray"}.get(val, "black")
    return f"color: {color}; font-weight: bold"

styled = df.style.set_table_attributes("style='display:inline'") \
    .format({'Predicted Class': color_class}) \
    .hide(axis="index") \
    .set_properties(subset=["Image"], **{'width': '70px'}) \
    .set_table_styles([{'selector': 'th', 'props': [('font-size', '110%'), ('text-align', 'center')]}])

# ✅ Display prediction table
IPython.display.display(IPython.display.HTML(styled.to_html(escape=False)))

# ✅ Save detailed predictions (excluding images) to Excel
df.drop(columns=["Image"]).to_excel("rotated_dog_cat_predictions.xlsx", index=False)

# ✅ Final Prediction Summary by Image
summary = []

for img_idx in df["Image Index"].unique():
    subset = df[df["Image Index"] == img_idx]
    dog_conf = subset[subset["Predicted Class"] == "Dog"]["Confidence (%)"].str.rstrip('%').astype(float)
    cat_conf = subset[subset["Predicted Class"] == "Cat"]["Confidence (%)"].str.rstrip('%').astype(float)
    none_conf = subset[subset["Predicted Class"] == "None"]["Confidence (%)"].str.rstrip('%').astype(float)

    avg_dog = dog_conf.mean() if not dog_conf.empty else 0
    avg_cat = cat_conf.mean() if not cat_conf.empty else 0
    avg_none = none_conf.mean() if not none_conf.empty else 0

    best_class = max(
        [("Dog", avg_dog), ("Cat", avg_cat), ("None", avg_none)],
        key=lambda x: x[1]
    )

    summary.append({
        "Image Index": img_idx,
        "Dog Avg Confidence": f"{avg_dog:.2f}%",
        "Cat Avg Confidence": f"{avg_cat:.2f}%",
        "None Avg Confidence": f"{avg_none:.2f}%",
        "Final Prediction": best_class[0],
        "Confidence (%)": f"{best_class[1]:.2f}%"
    })

# ✅ Display final summary
summary_df = pd.DataFrame(summary)
IPython.display.display(summary_df)

# ✅ Save final summary to Excel
summary_df.to_excel("final_prediction_summary.xlsx", index=False)

import matplotlib.pyplot as plt

# ✅ Bar chart for each image index
for i, row in summary_df.iterrows():
    labels = ['Dog', 'Cat', 'None']
    confidences = [
        float(row["Dog Avg Confidence"].rstrip('%')),
        float(row["Cat Avg Confidence"].rstrip('%')),
        float(row["None Avg Confidence"].rstrip('%'))
    ]

    plt.figure(figsize=(6, 4))
    bars = plt.bar(labels, confidences, color=['green', 'blue', 'gray'])
    plt.ylim(0, 100)
    plt.title(f"Image {int(row['Image Index'])} - Final Prediction: {row['Final Prediction']}")
    plt.ylabel("Average Confidence (%)")

    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 2, f"{yval:.1f}%", ha='center', fontsize=10)

    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()

# ✅ Pie chart of class count per image
for img_idx in df["Image Index"].unique():
    subset = df[df["Image Index"] == img_idx]
    counts = subset["Predicted Class"].value_counts()

    labels = counts.index.tolist()
    values = counts.values.tolist()
    colors = ['green' if lbl == "Dog" else 'blue' if lbl == "Cat" else 'gray' for lbl in labels]

    plt.figure(figsize=(5, 5))
    plt.pie(values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    plt.title(f"Image {img_idx} - Prediction Distribution (by Rotation)")
    plt.axis('equal')
    plt.show()

print(df.columns.tolist())

import matplotlib.pyplot as plt

# Group by 'Image Index' and plot confidence vs rotation
for img_idx in df["Image Index"].unique():
    subset = df[df["Image Index"] == img_idx]

    plt.figure(figsize=(8, 5))
    plt.plot(subset["Rotation"], subset["Confidence (%)"], marker='o', color='blue')

    plt.title(f"Image {img_idx} - Confidence vs. Rotation Angle")
    plt.xlabel("Rotation Angle (degrees)")
    plt.ylabel("Confidence (%)")
    plt.ylim(0, 100)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

# Install qgrid
!pip install qgrid --quiet

# Enable extension
import qgrid
import pandas as pd

# Make sure your DataFrame is already loaded as df
# For example: df = pd.read_excel("your_file.xlsx")

# Show interactive grid
qgrid_widget = qgrid.show_grid(df, show_toolbar=True)
qgrid_widget

import gradio as gr
from PIL import Image
import numpy as np

# Example predict function (replace with your actual model logic)
def predict_image(img):
    # Resize / preprocess
    img = img.resize((224, 224))
    img_array = np.array(img) / 255.0
    img_array = img_array.reshape(1, 224, 224, 3)

    # Predict using your model (replace this with real model)
    # For example: prediction = model.predict(img_array)
    # Simulated output for demo:
    predicted_label = "Cat"
    predicted_class = 1
    confidence = 92.5
    rotation_angle = "90°"

    return f"Label: {predicted_label}\nClass: {predicted_class}\nConfidence: {confidence}%\nRotation: {rotation_angle}"

# Build UI
gr.Interface(
    fn=predict_image,
    inputs=gr.Image(type="pil"),
    outputs="text",
    title="🐶🐱 Image Rotation & Prediction",
    description="Upload a rotated dog/cat image to get predictions"
).launch()